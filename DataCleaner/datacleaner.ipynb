{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a37621db6f99f75e4d2a47a248a6a71709dd01999a0a2eb01144ce85e79d4af1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \"Article Name\" from all \"Article Name.html\" for a list of article names\n",
    "file_names = [name[:-5] for name in listdir('../wikiarticles30') if isfile(join('../wikiarticles30', name))]\n",
    "file_names.remove(\".DS_\")\n",
    "# turn list into df\n",
    "wikiarticles = pd.DataFrame(file_names,columns=['article_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wiki_url_extractor(filename):\n",
    "    '''\n",
    "    Program to extract wgPageName from html of each site, then returns url\n",
    "    Looking through html file to get index for wgPageName in html\n",
    "    Then, iterating through the strings after to locate the next \", where the wgPageName ends\n",
    "    Finally, returns the functional wikipedia url\n",
    "    '''\n",
    "    html_handle = open(filename)\n",
    "    html_text = html_handle.read()\n",
    "    wgPageName_index = html_text.find('\"wgPageName\":')\n",
    "\n",
    "    # for loop to find the second quotation mark marking the end of wgPageName\n",
    "    end_index = 0\n",
    "    for i in range(wgPageName_index + 14, wgPageName_index + 500):\n",
    "        if html_text[i] == '\"':\n",
    "            end_index= i\n",
    "            break\n",
    "\n",
    "    wgPageName_raw = html_text[wgPageName_index+14:end_index]\n",
    "    wgPageName = wgPageName_raw.replace(\"'\", \"%27\") #replace ' with %27\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/\" + wgPageName\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/Slaughter_Trail\nhttps://en.wikipedia.org/wiki/Sonny_Dee_Bar\nhttps://en.wikipedia.org/wiki/Sproat_Lake\nhttps://en.wikipedia.org/wiki/Sissi_–_The_Young_Empress\nhttps://en.wikipedia.org/wiki/Spelungula\nhttps://en.wikipedia.org/wiki/Slab_City,_Wisconsin\nhttps://en.wikipedia.org/wiki/Sophia_Somajo\nhttps://en.wikipedia.org/wiki/Single_visit_dentistry\nhttps://en.wikipedia.org/wiki/Sredinny_Range\nhttps://en.wikipedia.org/wiki/Spill_containment\nhttps://en.wikipedia.org/wiki/South_Yorkshire_Fencing\nhttps://en.wikipedia.org/wiki/Spawn_(novel)\nhttps://en.wikipedia.org/wiki/Simpson%27s_Hospital,_Dublin\nhttps://en.wikipedia.org/wiki/Socket_(video_game)\nhttps://en.wikipedia.org/wiki/St_Clement%27s_Church,_Norwich\nhttps://en.wikipedia.org/wiki/Southern_Defense_Command\nhttps://en.wikipedia.org/wiki/Slovakia_men%27s_national_under-18_basketball_team\nhttps://en.wikipedia.org/wiki/St_Bonaventure%27s_High_School\nhttps://en.wikipedia.org/wiki/Sonargaon_Upazila\nhttps://en.wikipedia.org/wiki/Solomon%27s_Stone\nhttps://en.wikipedia.org/wiki/Sirab,_Hamadan\nhttps://en.wikipedia.org/wiki/Solec-Zdrój\nhttps://en.wikipedia.org/wiki/Sir_George_Tyler,_1st_Baronet\n"
    }
   ],
   "source": [
    "# add article url column for every article\n",
    "for article_name in wikiarticles['article_name']:\n",
    "    url_list = []\n",
    "    url = wiki_url_extractor('../wikiarticles30/' + article_name + '.html')\n",
    "    print(url)\n",
    "    url_list.append(url)\n",
    "    #wikiarticles['url'] = url_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}