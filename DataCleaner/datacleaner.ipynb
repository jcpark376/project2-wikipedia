{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a37621db6f99f75e4d2a47a248a6a71709dd01999a0a2eb01144ce85e79d4af1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \"Article Name\" from all \"Article Name.html\" for a list of article names\n",
    "file_names = [name[:-5] for name in listdir('../wikiarticles30') if isfile(join('../wikiarticles30', name))]\n",
    "file_names.remove(\".DS_\")\n",
    "# turn list into df\n",
    "wikiarticles = pd.DataFrame(file_names,columns=['article_name'])\n",
    "wikiarticles[\"url\"] = \"\" #empty column for url\n",
    "wikiarticles = wikiarticles.set_index('article_name') #set article name as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wiki_url_extractor(filename):\n",
    "    '''\n",
    "    Program to extract wgPageName from html of each site, then returns url\n",
    "    Looking through html file to get index for wgPageName in html\n",
    "    Then, iterating through the strings after to locate the next \", where the wgPageName ends\n",
    "    Finally, returns the functional wikipedia url\n",
    "    '''\n",
    "    html_handle = open(filename)\n",
    "    html_text = html_handle.read()\n",
    "    wgPageName_index = html_text.find('\"wgPageName\":')\n",
    "\n",
    "    # for loop to find the second quotation mark marking the end of wgPageName\n",
    "    end_index = 0\n",
    "    for i in range(wgPageName_index + 14, wgPageName_index + 500):\n",
    "        if html_text[i] == '\"':\n",
    "            end_index= i\n",
    "            break\n",
    "\n",
    "    wgPageName_raw = html_text[wgPageName_index+14:end_index]\n",
    "    wgPageName = wgPageName_raw.replace(\"'\", \"%27\") #replace ' with %27\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/\" + wgPageName\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add article url column value for every article\n",
    "for article_name in wikiarticles.index:\n",
    "    url = wiki_url_extractor('../wikiarticles30/' + article_name + '.html')\n",
    "    url_list.append(url)\n",
    "    wikiarticles.loc[article_name, 'url'] = url"
   ]
  }
 ]
}