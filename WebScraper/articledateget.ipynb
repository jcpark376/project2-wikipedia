{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a37621db6f99f75e4d2a47a248a6a71709dd01999a0a2eb01144ce85e79d4af1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract \"Article Name\" from all \"Article Name.html\" for a list of article names\n",
    "file_names = [name[:-5] for name in listdir('../wikiarticles') if isfile(join('../wikiarticles', name))]\n",
    "file_names.remove(\".DS_\")\n",
    "# turn list into df\n",
    "wikiarticles = pd.DataFrame(file_names,columns=['article_name'])\n",
    "wikiarticles = wikiarticles.set_index('article_name') #set article name as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wiki_url_firstdate_extractor(filename):\n",
    "    '''\n",
    "    Program to extract wgPageName from html of each site, then returns url\n",
    "    Looking through html file to get index for wgPageName in html\n",
    "    Then, iterating through the strings after to locate the next \", where the wgPageName ends\n",
    "    Finally, returns the functional wikipedia url that redirects to the history page\n",
    "    when article was first created\n",
    "    '''\n",
    "    html_handle = open(filename)\n",
    "    html_text = html_handle.read()\n",
    "    wgPageName_index = html_text.find('\"wgPageName\":')\n",
    "\n",
    "    # for loop to find the second quotation mark marking the end of wgPageName\n",
    "    end_index = 0\n",
    "    for i in range(wgPageName_index + 14, wgPageName_index + 500):\n",
    "        if html_text[i] == '\"':\n",
    "            end_index= i\n",
    "            break\n",
    "\n",
    "    wgPageName_raw = html_text[wgPageName_index+14:end_index]\n",
    "    wgPageName = wgPageName_raw.replace(\"%\", \"%26\") #replace % with %26\n",
    "    wgPageName = wgPageName_raw.replace(\"'\", \"%27\") #replace ' with %27\n",
    "    \n",
    "    \n",
    "    url = \"https://en.wikipedia.org/w/index.php?title=\" + wgPageName + \"&dir=prev&action=history\"\n",
    "    return url"
   ]
  },
  {
   "source": [
    "# For each article, there's a history page.\n",
    "# This loop goes to the first date of article's creation and gets the value\n",
    "# Add resulting column (full of article creation dates) to dataframe\n",
    "exec_counter = 0\n",
    "create_date = {}\n",
    "for article_name in wikiarticles.index:\n",
    "    url = wiki_url_firstdate_extractor('../wikiarticles/' + article_name + '.html')\n",
    "    temp = requests.get(url)\n",
    "    soup = BeautifulSoup(temp.text)\n",
    "    try: #on some articles I'm getting an error\n",
    "        date = soup.find_all('a', attrs={\"class\":\"mw-changeslist-date\"})[-1].get_text()\n",
    "        create_date[date] = [article_name]\n",
    "    except: \n",
    "        pass\n",
    "    exec_counter += 1\n",
    "    print(exec_counter)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop rows without dates\n",
    "articles_with_dates = create_date.values()\n",
    "for file_name in file_names:\n",
    "    if [file_name] not in articles_with_dates:\n",
    "        wikiarticles.drop(index = file_name, inplace=True)"
   ]
  },
  {
   "source": [
    "# add dictionary to create new column in master df\n",
    "wikiarticles['date_article_creation'] = create_date\n",
    "\n",
    "# Save current dataframe to csv for next step\n",
    "wikiarticles.to_csv(r'wikiarticles.csv')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 25,
   "outputs": []
  }
 ]
}